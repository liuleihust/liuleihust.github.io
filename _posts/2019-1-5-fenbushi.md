---
layout:     post
title:      "InnoDB-分布式数据库"
author:     "liulei"
header-img: "img/post-bg-miui6.jpg"
tags:
    - InnoDB
---

# 分布式数据库

关系数据库系统中，处理事务的过程通常被视为一种分层的行为。系统在顶层对SQL语句进行解析，然后将得到的语法树传递给查询优化器层。查询优化器利用启发式规则和统计信息为每个关系操作符选取最优的策略。这个阶段产生的物理执行计划与逻辑存储层交互，完成相应的操作。 



在本文中，将事务处理引擎简化为两层模型：查询解析、查询执行以及查询优化视为查询处理引擎（Process Engine，PE）；逻辑层存储和物理层存储统称存储引擎（Storage Engine，SE）。这对应MySQL可插拔存储的两层架构。 

定义数据库服务器集群的架构决策的关键点在于集群共享发生的程度，它定义协调动作发生在什么层以及哪个层（PE和SE）将被复制或者共享。这不仅确定了系统在可扩展性和灵活性上的权衡，而且关系到每一种架构在现成的数据库服务器上的适用性。以下是四个具有代表性的架构： 

![](http://img.mp.itc.cn/upload/20170525/c1ddb15a8e4e483eb10618956cc61827_th.jpg)

## **Shared Disk Failover (SDF)** 

将 应用程序或服务安装在配置为发生故障时彼此接管对方工作的多台服务器上。 一台服务器接管发生故障的服务器的过程通常称为"故障转移"。  故障转移群集是一组这样配置的服务器：**如果一台服务器变为不可用，则另一台服务器自动接管发生故障的服务器并继续处理任务**。 **群集**中的每台服务器将群集中至少一台其他服务器确定为其备用服务器。 

### 检测故障

要让备用服务器变成活动服务器，它必须设法确定活动服务器不再正常工作。 通常，系统使用下列某个常规类型的心跳机制来做到这一点： 

- **发送信号**：对于发送信号，活动服务器以定义好的时间间隔将指定信号发送到备用服务器。如果备用服务器在**某个时间间隔**内未收到信号，则确定活动服务器发生了故障并取得活动角色。  
- **接收信号**: 对于接受信号，备用服务器向活动服务器发生请求。如果活动服务器没有响应，则备用服务器按特点次数重复发生此请求。如果活动服务器仍然没有响应，则备用服务器接管活动服务器的工作。  

群集可以使用多个级别的信号。 例如，群集可以在服务器级别使用发送信号，并在应用程序级别使用一组接收信号。 在此配置中，每当活动服务器启动并连接到网络时它都将心跳消息发送到备用服务器。 这些心跳消息是按比较频繁的时间间隔（例如每隔 5 秒）发送的，而备用服务器可能通过编程设置为如果仅仅未收到两个心跳消息，它就接管活动服务器的工作。 也就是说，在活动服务器发生故障后不超过 10 秒的时间内，备用服务器将检测到这一故障并启动备用进程。 

相当常见的情况是，信号是通过专用通信通道发送的，以便网络拥塞和一般网 络问题不会导致假的故障转移。 此外，备用服务器可能将查询消息发送到运行在活动服务器上的一个或多个关键应用程序，并在指定的超时间隔内等待响应。 如果备用服务器收到正确的响应，则它不采取任何进一步行动。 为了将对活动服务器的性能影响减少到最小，应用程序级别的查询通常要经过比较长的时段，如每隔一分钟或更长。 备用服务器可能通过编程设置为：一直等到至少已经发送五次请求但未收到响应，然后才接管活动服务器。 这意味着，可能在长达 5 分钟之后，备用服务器才会启动故障转移进程。 

### 同步状态

备用服务器必须首先将其状态与发生故障的服务器的状态进行同步，然后才能开始处理事务。 主要有三种不同的同步方法： 

- **事务日志**：在事务日志方法中，，活动服务器将其状态的所有更改记录到日志中。 一个同步实用工具定期处理此日志，以更新备用服务器的状态，使其与活动服务器的状态一致。 当活动服务器发生故障时，备用服务器必须使用此同步实用工具处理自上次更新以来事务日志中的任何添加内容。 在对状态进行同步之后，备用服务器就成为活动服务器，并开始处理事务。 
- **热备用**：在热备用方法中，将把活动服务器内部状态的更新立即复制到备用服务器。 因为备用服务器的状态是活动服务器状态的克隆，所以备用服务器可以立即成为活动服务器，并开始处理事务。 
- **共享存储：**在共享存储方法中，两台服务器都在共享存储设备（如存储区域网络或双主机磁盘阵列）上记录其状态。 这样，因为不需要进行状态同步，故障转移可以立即发生。 

#### 存储区域网络（SAN）

> 一种专门为存储建立的独立于TCP/IP 网络之外的专用网络。

![](http://5b0988e595225.cdn.sohucs.com/images/20180109/0459d5cb81554280a1d123ff2b12a13f.jpeg)

SAN 采用网状通道技术，通过FC（Fibre Channel）交换机连接存储阵列和服务器主机，建立专用于数据存储的区域网络。



SAN 网络最初主要指**FC-SAN** （通过光纤通道协议转发SCSI协议），目前常见的还有**IP-SAN** （通过TCP协议转发SCSI协议 ）。**SCSI**(Small Computer System Interface )小型计算机系统接口，一种用于计算机和智能设备之间（硬盘，软驱，光驱，等）系统级接口的独立处理器标准。SCSI是一种智能的通用接口标准。



**FC-SAN**：多个主机通过光纤交换机连接后端存储和带库等设备。

##### SAN 网络的组成

SAN网络由：主机层，交换层，存储层。另外还有SAN 网络的监控。

- **主机层**：SAN 网络最终目的是为了上层业务系统搭建一个支撑平台，存储数据。因为业务系统 是OS上面 ，底层的硬件部分就是主机部分，不管是虚拟化环境还是物理机。直接与SAN网络打交道的还是要通过物理硬件连接。需要配置主机的端的HBA卡 。HBA 卡连接光纤交换机需要光纤线。
- **交换层**：核心部件是FC光纤交换机和SFP模块。一个小型的SAN网络基本通过一个或几个光纤交换机独立工作或者级联就可以支撑整个公司的SAN 网络环境，在一个特大型的SAN网络当中只靠光纤交换机的级联就显得非常的低效和故障隐患太高，这时就需要一个路由器（router），大型TCP/IP 网络需要路由器，大型的SAN网络同样也需要路由器，通过路由的功能来实现交换机之间的互联与通讯，进行形成一个大型和复杂的网络。 
- **存储层**：存储层只是一个概括而已，主要是指连接在光纤交换机上用于提供数据存放的设备，如存储设备，磁带库设备，NAS设备等。存储设备大多都有2到多个控制器，控制器通过光纤设备连接到光纤交换机，在交换机上配置相应的Zone,从而识别主机，映射到主机，最终完成主机设备存储，带库等相关设备的操作。 在SAN网络挡住主要就是连接到Switch，识别到主机，划分LUN映射给主机。

**LUN** ：Logical Unit Number 逻辑单元号。 LUN 不等于某个设备，只是个号码而已。

我们有了独立的**磁盘阵列**用了之后，服务器只要看到存储的控制系统，就有可能使用磁盘阵列的磁盘资源，但是磁盘阵列不可能只为某一个服务器来使用，所以它必须管制[主机](https://baike.baidu.com/item/%E4%B8%BB%E6%9C%BA)使用某部分磁盘资源。这个管制分为两个部分：一部分就是LUN mapping，类似于绿色通道，就是保证服务器能看到某部分存储资源，一部分就是LUN masking，类似于警戒线，就是保证服务器只可访问给它分配的存储资源，而没分配给服务器的资源，就不要染指了。 

### 确定活动服务器

对 于指定的一组应用程序，只存在一台活动服务器，这是极其重要的。 如果多台服务器都像是活动服务器，则通常会导致数据损坏和死锁。 解决此问题的常见方法是使用"活动令牌"概念的某个变体。 令牌在其最简单级别上是一个标志，用来将服务器标识为某个应用程序的活动服务器。 对于每组应用程序来说，只存在一个活动令牌；因此，只有一台服务器可以拥有令牌。 当服务器启动时，它会验证其合作伙伴是否拥有活动令牌。 如果拥有，则该服务器将作为备用服务器启动。 如果它未检测到活动令牌，则它会取得活动令牌的所有权，并作为活动服务器启动。 当备用服务器成为活动服务器时，故障转移进程将把活动令牌交给备用服务器。 

在大多数情况下，当备用服务器成为活动服务器时，对于它正在支 持的应用程序或用户来说，它是透明的。 如果在事务过程中发生了故障，则可能必须重试该事务以使其成功完成。 这就使编写应用程序代码时使故障转移进程保持透明显得更为重要。 这样做的一个示例是，在将数据提交到数据库时，包括重试和超时。 

此外， 大多数服务器使用 Internet 协议 (IP) 地址进行通信，因此，为了使故障转移成功，基础结构必须能够支持将 IP 地址从一台服务器转移到另一台服务器。



*Failover Cluster* 模式具有的优缺点 ：

 **优点**

- **适应计划内的停机时间。** 故障转移群集可以允许系统有停机时间，而不会影响可用性。 这样，就适应了日常的维护和升级需要。
- **减少计划外停机时间。** 故障转移群集通过消除系统和应用程序级别上的故障单点，减少了与服务器和软件故障有关的应用程序停机时间。

**缺点**

- **会增加响应时间。** 对于故障转移群集设计来说，由于备用服务器上的负载增长，或需要更新多台服务器的状态信息，因此会增加响应时间。

**增加设备成本。** 故障转移群集所要求的额外硬件很容易使基础结构层的成本加倍。

## **Shared Disk Parallel (SDP)** 

例子 ：

ORACLE 的 RAC。

![](https://img-blog.csdn.net/20180516200338534)



一般每个实例都放在不同的服务器上面，这样可以起到冗余作用。所有的数据库文件都放在共享存储上面，但是还有一些文件放在每个实例自己的本地的磁盘上面，比如参数文件，每一个实例都可以有自己的参数文件，这个参数文件既可以放在本地也可以放在共享存储上面，多个实例都使用同一个参数文件。

在RAC里面，最重要的就是实例和实例之间的交互，即使是分离的实例，但是读取的数据是相同的，**RAC不是分布式的系统，因为它只有一个存储，分布式系统是指数据发布在不同的数据库上面，然后通过中间件来协调做查询。RAC还是一台数据库，多个实例。** 

对于RAC来说至少**有两套物理上不同的网络**，私有网络是专门用来实例之间的数据交互。如果私有网络，所有的数据都在一个网络下面，那么那么就会对数据造成影响，严重的影响RAC的性能了。实例之间数据之间传递使用私有网络和对外服务提供的网络之间是物理分开的。所以RAC至少有两套网络，**一个是实例之间的数据的传递**，另外**一个是公有网络，是对外提供服务的**，外面的业务是提供公有网络的IP链接到数据库的。

#### RAC的特点



除了具有普通的数据库特性外：

- 每一个节点的instance都有自己的SGA
-  每一个节点的instance都有自己的background process
-  每一个节点的instance都有自己的redo logs
-  每一个节点的instance都有自己的undo表空间 

每一个节点的实例都有自己的SGA，但是之间在SGA里面的数据块都是需要相互传递的。

每一个节点都有自己的redo,redo不是共用的。虽然redo是放在共享磁盘上面，但是每个实例都有自己的redo，各有各的。当实例2坏了，实例1做恢复的时候可以通过实例2的redo信息来进行恢复。

每个实例都要处理自己的一套事务，所以需要使用自己的UNDO。

所以在RAC架构下面，每一套实例都有自己的东西。



**RAC如何容错：**

![](https://img-blog.csdn.net/20180516200435248)

当使用SAN 存储，存储和服务器就不在一起了，而是和服务器分离了，是一个网络的存储系统，服务器是服务器，存储是存储，完全分离的。

当节点1坏了，不仅仅只是将业务切换到节点2，这个时候还要处理节点1坏了遗留下来的问题。主要是一些提交和未提交的事务。比如实例1坏了，实例2就要从实例1的redo里面读最后一次checkpoint之后的信息，就是实例1最后将所有的数据写到磁盘之后产生的所有的redo应用，该回滚的回滚，该恢复的恢复。即现将以提交和未提交的事务产生的redo都先应用，因为是最后一次checkpoint之后的信息，之前的信息是已经写到磁盘上面了，所谓的恢复是将未写到磁盘上面的信息进行恢复。（虽然实例坏了，但是实例的redo并没有坏，放置在共享存储上面，它的redo还是允许其他实例访问的）实例2读取实例1的redo，然后全部应用一遍，之后再回滚未提交的事务修改的数据块，这个有点像实例的恢复。）



这种架构主要是针对服务器可用的CPU和内存带宽上进行扩展。它提供与SDF等同的容错能力，因为大多数服务堆栈在面对更新事务的时候仍然没有被复制。 



## **Shared Nothing Active（SNA）** 

通过完全隔离后端服务器，中间层拦截所有客户机请求并将其转发到独立副本。因为只读请求在可用节点之间得到平衡从而达到系统可扩展。因此，只有更新事务需要积极地在所有副本进行复制。控制器扮演包装器的角色。对于请求者而言，它充当服务器，提供相同的客户端接口。对于原始的服务器而言，该中间层作为客户端。群集节点之间没有直接的沟通，因为协调发生在服务器之外 。

可扩展性上的主要缺点就是更新语句必须是完全确定的，并且需要小心调度以避免冲突从而导致非确定的执行结果和数据库不一致。 实际上，这通常意味着不允许并发执行更新事务。这个架构的目标主要还是增强服务器在面对大多以读为主的工作负载能力。通过完全隔离后端服务器，在处理更新事务的时候，它在所有数据库系统的软件层重复执行，从而容许PE和SE的故障情况。事实上，可移植的实现，例如Sequoia，甚至支持多样的DBMS。它甚至可以在崩溃的状态上进行投票，以掩盖错误的备份。





## **Shared Nothing Certification-Based (SNCB)** 

在无共享集群，可以使用基于认证协议避免主动复制更新事务。每个事务直接在副本上执行，而没有任何先验协调。因此，事务仅需要根据本地的并发控制协议在本地进行同步。仅在提交之前，协调的过程才开始。这个时候，发起协调的副本采用全序组通信原语广播更新。这将导致所有节点采用完全相同的更新序列，然后通过测试可能的冲突获取认证。 它能够随着更新密集型工作负载的扩展而执行更细粒度的同步。这方法能够接受存储引擎和磁盘层的物理损坏。 





## Aurora

基本概念：

- 数据中心：单独存放数据的机房。
- 可用区：具有独立的供电、独立的网络。一个可用区一般由多个数据中心组成。
- 区域：亚马逊全球总共十个区域，一个区域包含多个可用区。

![](/img/in-post/innoDB_img/360截图18770522432883.jpg)





AWS的每个区域一般由多个可用区（AZ）组成，而一个可用区一般是由多个数据中心组成。AWS引入可用区设计主要是为了提升用户应用程序的高可用性。因为可用区与可用区之间在设计上是相互独立的，也就是说它们会有独立的供电、独立的网络等，这样假如一个可用区出现问题时也不会影响另外的可用区。在一个区域内，可用区与可用区之间是通过高速网络连接，从而保证有很低的延时。

### Aurora架构设计

> 其体系结构类似SDP，但是它将更新局限在一个DB服务器上，避免了分布式并发控制协议。

传统的数据库实现可扩展本质上都在数据库的不同层面耦合（SNA 在应用层，分布式（SNCB）在SQL层，SDP在缓存层），扩展后的每个实例的程序栈仍然是原来的多层结构。Aurora认为从成本、部署灵活性及可用性等因素考虑，**应该考虑把数据库的各层打开，然后在每个层单独做扩展。** 传统的数据库系统，例如MySQL、PostgreSQL以及Oracle，将所有的功能模块封装成一个整体，而Aurora则是将数据库的**缓冲区管理、恢复子系统从这个整体剥离出来，单独定制扩展。** 

![](https://mmbiz.qpic.cn/mmbiz_png/5DetbrlibzecunM06l8E2nibDiaib1pK7ZLuhe6cdCBMSu7aLVwSn2GD2tJ3Av3E71ibia74p570iauXdbCuHy7Gjz33w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

该图表示 Aurora 将 计算层（sql 语句处理） 与 存储层 分离。**查询处理**、**事务**、**缓存**等数据库的核心功能都还是在计算层的mysql实例中处理，但是**持久化、备份恢复**等能力都已经移到了S3（亚马逊云服务平台）上。 

相比传统的数据库系统架构，Aurora具有以下三个明显的优点。

- 首先，在跨数据中心环境将**存储作为一个独立、具有容错以及自修复能力的服务模块，使得数据库系统免受性能抖动和存储或者网络故障的干扰。**Aurora设计人员观察到持久层的故障可以认为是**系统长时间不可用事件**，而系统不可用事件又可以建模为长时间的系统性能抖动。一个设计良好的系统可以无差别地处理这些问题。也就是，**Aurora通过底层可靠的存储系统保证数据库系统的服务层级**（Service Level Agreement, SLA）。 
- Aurora的设计理念是**日志即数据**。通过只将重做日志记录写入存储层，系统可以将网络的**IOPS减少一个数据量级**。一旦移除了网络I/O瓶颈，在MySQL的代码基础上可以针对各种资源竞争进行大量优化，获得大幅的性能提升。 
- 将数据库系统中一度被认为**最复杂与关键**的功能（例如重做、备份等）委托给底层的分布式存储。存储系统以**异步方式**持续在后台并行构造最新版本的数据。这使得Aurora可以达到即时恢复的效果。 

**存储与计算分离**，对于Aurora来说，并不是一个选择题。在Amazon生态下，存储本来就是和计算分离的，从逻辑上看，可以认为系统有一个巨大的共享存储（或许使用的共享存储就是计算节点的本地存储）。Aurora能够做的事情，就是尽可能减少计算与存储之间的带宽需求，这是整个架构的关键所在 

Aurora通过**只传输重做日志记录**以消除不必要的IO操作，降低成本，**并确保资源可服务于读/写流量**。与传统的数据库引擎不同，Amazon Aurora不会将修改后的数据库页面推送到存储层，进一步节省了IO消耗。 



**Aurora术语定义** 

| **术语**                    | **相应解释**                                                 |
| --------------------------- | ------------------------------------------------------------ |
| AZ（Availability Zone）     | 可用数据中心，Aurora中所有AZ位于同一个region                 |
| LSN（Log Sequence Number）  | 日志序列号，数据库系统为每个日志记录生成的唯一记录ID。传统的数据库系统采用文件偏移量表示，在Aurora中利用时间戳标记。 |
| VCL（Volume Complete LSN）  | 存储节点接收到的最大连续日志ID，这些日志可能还未提交成功。   |
| SCL (Segment Complete LSN)  | Aurora中数据分片对应的最大连续日志ID，节点利用该变量与其它节点交互，填补丢失的日志记录 |
| CPL (consistency Point LSN) | 在数据库层面，事务被分成多个**MTRs**（Min-Transactions）。每个MTR产生的最后一个LSN为一个CPL。 |
| VDL (Volumn Durable LSN)    | VDL为最大的CPL，其中CPL ≤ VCL;在系统恢复阶段需要通过多数派读确定VDL |

#### 降低写次数

传统数据库具有写放大问题，也就是本来一条记录，到数据库层面，要写各种日志（redo,undo,binlog），数据要写4次，至少要进行四次网络IO，并且有3次是串行的，这就是性能差的原因。 Aurora 的整个方案可以理解为CQRS模式，也就是写的时候只要写入队列即返回，读的时候走缓存。



![](https://mmbiz.qpic.cn/mmbiz_jpg/5DetbrlibzecunM06l8E2nibDiaib1pK7ZLuovzdcfd0lPkNCATdwiccd92h8ZKvDwGibpwfQEGE9ibsia87WW2f6MDFzQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如图所示，它涉及以下步骤：

1. 接收日志记录并添加到内存中队列
2. 持久化记录，并给客户端返回
3. 归类日志，并确认有哪些丢失了。
4. 通过GOSSIP协议与其他节点交互
5. 将日志记录合并到新数据页
6. 定期将日志和新页存储到S3
7. 定期垃圾收集旧版本
8. 定期验证页面上的CRC码。

只有1,2 步需要串行，后面都是异步的。

#### 实现扩展性及高可用



![](https://mmbiz.qpic.cn/mmbiz_jpg/5DetbrlibzecunM06l8E2nibDiaib1pK7ZLubb95DS7oHKL2zVnnOuZ9ia0chcrpVPP0tAdZXpCUOxkP5G9hHa6VVXw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如图所示，Aurora由3个跨AZ（可以理解为同城的数据中心）的实例组成，一个主实例和多个副本实例（可以扩展到15个），写只能通过主实例，读则可以通过任何实例进行，主实例与副本实例或者存储节点间**只传递redo日志**和元信息。 每个AZ存储两个副本，主实例并发向6个存储节点和副本实例发送日志，当4/6的存储节点应答后，则认为日志已经持久化，不必等待6个节点全部返回。 



Aurora的设计目标是要满足以下场景：

1. 如果三个AZ中有一个AZ挂掉还可以写  
2. 如果三个AZ中有一个AZ挂掉，另外两个没有挂掉的AZ中有一个实例挂掉了，不影响读，且不会丢失数据  。

很明显，根据Quorum协议，是满足要求的，W=4，R=3，N=6，满足W+R>N，且满足2W>N,是为了解决写冲突，**两次写入必须有交集**，这样才能在写的过程中解决写冲突。 





## 论文

 Amazon Aurora 是一个Amazon Web Service (AWS)中的一部分，它是一个为了OLTP（联机事务处理）的关系数据库服务。在本文中，我们将描述Aurora的体系结构以及如何这样考虑的。在高数据量处理的背景下，我们相信主要的约束已经从计算和存储转移到了网络(也就是网络IO成了约束高性能数据库的主要因素）。Aurora为了解决这个约束，为关系型数据库带来了一个新的体系结构，主要是将redo进程分发到存储服务。我们介绍了怎样不仅减少网络阻塞，而且允许快速的冲突恢复、在没有丢失数据的情况下启用备份，具有容错性，自我修复能力的存储。然后我们将介绍Aurora如何通过多个存储节点利用高效的异步模式来实现持久化的一致性，而且避免了繁琐的恢复协议。最后，我们已经将Aurora作为产品服务运行了18个月，我们将分享了一些经验从我们客户那里与云应用得到的。

### 介绍

